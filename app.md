Adds-App is a React Native (Expo) mobile app that uses Google’s Gemini AI to generate advertisement images based on user input (text or images). The app features a feed of static template ads and a bookmarking system for saving favorite results. Currently, the app has no database (feed items are manually curated and stored in the app), but we plan to add a backend for persistence and scalability. This guide will explain how to architect Adds-App with Supabase as the backend database (PostgreSQL), Clerk for user authentication, and Redis for caching. We’ll cover setting up each component, modeling the data (user accounts, saved images, generation history), and when to use caching for performance. The goal is a beginner-friendly, structured walkthrough of this stack.
Architecture Overview
At a high level, our app’s backend stack will consist of:
Supabase (PostgreSQL database) – The primary backend for storing data (users, ads, images, history). Supabase is an open-source Firebase alternative built around a Postgres database
supabase.com
. It provides out-of-the-box features like RESTful and realtime APIs, row-level security (RLS) for access control, file storage, and serverless functions, all powered by standard Postgres.
Clerk (Authentication service) – A hosted user authentication service that manages sign-ups, logins, and user accounts. We’ll use Clerk’s prebuilt UI components and identity management, and then integrate Clerk with Supabase so that each request to the database is authorized as a specific user. Clerk will issue JSON Web Tokens (JWTs) for logged-in users, which Supabase can validate to enforce security.
Redis (In-memory cache) – An optional caching layer to improve performance for certain data. Redis is an in-memory key–value store often used as a cache
supabase.com
. We plan to use it to speed up read-heavy or static data like the feed of template images, and possibly to cache expensive AI generation results. This helps the app’s feed load quickly for ~1,000 users (our first-week target) and reduces load on the database.
Google Gemini AI (External service) – The AI model (hosted by Google) that generates advertisement images from user input. This isn’t part of our stack per se, but it integrates with our backend. We might call Gemini’s API from a secure environment (like a Supabase Edge Function or our own server) and then store the results in Supabase (with optional caching in Redis).
Below is a simplified flow of how these pieces interact in the Adds-App architecture:
User Authentication: A user signs up or logs in via Clerk (e.g., using email/password or OAuth). Clerk handles all user credentials and sessions. Once authenticated, the mobile app receives a session token (JWT) from Clerk.
Supabase Client (in Expo App): The Expo app creates a Supabase client instance with the project URL and an anon public key. We configure this client to use the Clerk JWT for authorization on every request. This means each Supabase query from the app will include the user’s JWT in the header
clerk.com
, allowing Supabase to know which user is making the request.
Database Operations: Supabase provides an auto-generated RESTful API (and client libraries) for the Postgres database. We will design tables for our data (feed templates, generated ads, bookmarks, etc.). Thanks to row-level security policies tied to the JWT, Supabase will ensure each user can only access or modify their own data
clerk.com
clerk.com
. For example, when the app requests “get my saved ads,” Supabase checks the token’s user ID against a user_id field on the records and only returns matches. All database reads/writes (like saving a bookmark or logging a generation event) go through Supabase’s API. Supabase does not cache these API responses by itself
github.com
, so each request hits the database (Postgres does have internal caching of queries, but no app-level caching).
File Storage: Images (the template images and generated ad images) will be stored in Supabase’s Storage service (which wraps cloud object storage). Instead of storing large image blobs directly in the Postgres tables, we’ll store them in Storage buckets and save only the file URL or path in our tables. This keeps the database lean – it stores references and metadata, while the heavy image files are served via Supabase’s CDN.
Feed Delivery (with Redis): When the app loads the feed of template ads, it needs to fetch a list of these template images (and associated info). Since this feed is the same for all users and changes rarely (manually curated), we can use Redis to cache it. For instance, the first time the feed is requested, our server (or an edge function) can retrieve the list from Supabase, then store that list in Redis. Subsequent requests can then fetch the feed data quickly from Redis memory instead of hitting the database every time. This results in faster load times for the feed and reduces database load. If we update the feed (add/remove templates), we would also update or invalidate the Redis cache to keep it in sync.
AI Ad Generation Workflow: When a user selects a template and provides input (e.g. uploads their own image or enters text), the app will call an API to generate the new advertisement via Google’s Gemini AI. Typically, we don’t call such AI APIs directly from the client (to avoid exposing API keys and because it may take time). Instead, we can create a Supabase Edge Function (a serverless function running on Supabase’s infrastructure) or use a separate backend server. This function would: receive the user’s request (including their input and chosen template), call the Gemini AI API, get the generated ad image, and then store the result – e.g. upload the image to Supabase Storage and insert a record into the “generated_ads” table (with fields like user_id, template_id, etc.). Finally, the function returns the result (perhaps the image URL or ID) to the mobile app. We might also leverage Redis here: for example, caching the result for a short time in case the user makes the same request again or to quickly retrieve the image after generation. (Caching the AI results is optional and depends on usage patterns — more on this in the caching section.)
Bookmarking & History: As users browse the feed or generate new ads, they can bookmark images they like. For feed templates, bookmarking might simply mark them as favorites for quick access. For generated ads, a bookmark means the user wants to save that generated result in their favorites. In both cases, a new record will be created in a “bookmarks” table linking the user’s ID and the image (either referencing a template or a generated ad). The app can later retrieve “My Bookmarked Images” by querying this table (which will again be automatically filtered to the current user via RLS). All of this data (bookmark links, history entries, etc.) resides in Supabase.
In summary, Supabase is our reliable source of truth for data and files, Clerk provides secure user accounts (with Supabase enforcing per-user data access), and Redis is a helper for speeding up certain reads. Next, we’ll dive deeper into setting up each part and modeling the data.
Setting Up Supabase as the Backend Database
1. Create a Supabase Project: Start by creating a project in Supabase (via their website). This provisions a Postgres database and the Supabase services (auth, storage, etc.) for your app. Note the Project URL (API endpoint) and the anon API key – we’ll use these in the app. (The anon key is a public key used by clients; it has limited rights defined by the database’s RLS policies.) 2. Define the Database Schema: In the Supabase dashboard (or using SQL), create the tables needed for Adds-App. We propose the following tables and structures for the main features:
Templates Table (Feed): Stores the static ad template data shown in the feed. Columns might include: id (primary key), image_url (or a storage path) for the template image, title or description of the template, and any other metadata (e.g., category or tags). Since these are not user-specific, we don’t necessarily need a user_id on this table (they are global content).
Generated_Ads Table (Ad Generation History): Stores records of each ad generated by users via the AI. Columns: id (PK), user_id (the ID of the user who generated it), template_id (which template was the base, if any), input_text or input_image_path (the user’s input to Gemini, if we want to log it), output_image_url (path to the generated ad image in storage), created_at timestamp, etc. The user_id will link to the Clerk user. We will secure this table so users can only see their own entries. Because images can be large, we store output_image_url rather than the binary image itself – the actual image file will live in Supabase Storage (or an external URL).
Bookmarks Table (Saved Images): Lets users save/bookmark images (either templates or generated ads). We can design this in a couple of ways. One simple approach is a generic bookmarks table: id (PK), user_id, image_type (enum or text indicating “template” vs “generated”), image_ref_id (the ID of the template or generated_ad that’s bookmarked), plus perhaps a created_at. For instance, if a user bookmarks template #5, we insert a row with user_id = ..., image_type = 'template', image_ref_id = 5. If they bookmark one of their generated ads (say id 17 in Generated_Ads), we insert image_type = 'generated', image_ref_id = 17. This design centralizes bookmarks in one table. Alternatively, you could have separate bookmark tables for templates and generated ads, but that’s more schema to maintain – the unified approach is fine as long as we know how to interpret the image_ref_id.
(Optional) User Profile Table: Since Clerk manages user identity, we might not need a user profile table in Supabase at all, unless we want to store additional info per user (like a username, profile photo, or app-specific settings). Clerk provides the basic user info (ID, email, etc.) and we can always fetch that from Clerk when needed. If we do create a users table in Supabase, it could be to mirror some Clerk info or extend it. But an alternative is to just use the Clerk user ID everywhere (as we do in user_id fields) without a dedicated table. For this guide, we’ll assume we don’t need a separate users table (to keep things simple and rely on Clerk for user identities).
3. Enable Row Level Security (RLS): By default, Supabase projects have RLS disabled on new tables (meaning any request with a valid anon or service key can read/write the table). We want to enable RLS on any table that contains user-specific data, such as Generated_Ads and Bookmarks, to prevent users from accessing each other’s data. In the Supabase dashboard, toggle RLS “on” for those tables. Then, add policies that allow each user to interact with only their rows. For example, for the Generated_Ads table, we create policies: one for SELECT (read) and one for INSERT (create). The SELECT policy might say: user can select a row if user_id = auth.jwt()->> 'sub', i.e., the row’s user_id matches the user ID from the JWT’s subject (sub) claim
clerk.com
. Similarly, an INSERT policy can ensure when a user adds a new record, that record’s user_id must match their own ID
clerk.com
. We can use a Postgres function auth.jwt() to access the JWT claims provided by Clerk
clerk.com
. Clerk’s integration will include the user’s ID as the JWT sub (subject). By using this in RLS, Supabase ensures “users can only access data that belongs to them”
clerk.com
. (The Supabase docs and Clerk docs provide examples of these exact policies when integrating Clerk.) Don’t worry if this sounds complex – in practice, you write the policy once, and Supabase enforces it on every query automatically. 4. Integrate Supabase with Clerk: Now that our tables are ready to receive data securely, we need to make sure Supabase recognizes Clerk’s authentication. Supabase allows “Third-Party Auth” providers (TPA). In the Supabase dashboard, under Authentication > Third Party Auth, enable Clerk and provide your Clerk instance’s domain (from the Clerk dashboard)
clerk.com
. This essentially tells Supabase to trust JWTs issued by your Clerk instance. Clerk will also include a special claim in its JWTs (like "role": "authenticated") so that Supabase knows the user is authenticated
clerk.com
clerk.com
. Once configured, any request to Supabase’s API with a valid Clerk JWT will be treated as an authenticated user. 5. Connect from the React Native App: In your Expo app, install the Supabase JavaScript SDK (@supabase/supabase-js) and the Clerk React Native SDK. You’ll initialize Clerk (which handles user login flows) and Supabase in your app’s startup. The key part is to supply Supabase with the Clerk JWT for each request. With the Supabase JS client, you can do something like:
js
Copy
Edit
import * as SecureStore from 'expo-secure-store'; // for token storage if needed
import AsyncStorage from '@react-native-async-storage/async-storage';
import { createClient } from '@supabase/supabase-js';
import { Clerk } from '@clerk/clerk-expo';

// Initialize Clerk (with your Clerk frontend API key)
Clerk.initialize(...);

// Create Supabase client
const supabaseUrl = "https://YOUR_PROJECT_ID.supabase.co";
const supabaseAnonKey = "YOUR_SUPABASE_ANON_KEY";
export const supabase = createClient(supabaseUrl, supabaseAnonKey, {
  auth: {
    // Use AsyncStorage for React Native to store any auth state if Supabase needed it
    storage: AsyncStorage,
  },
  global: {
    headers: {
      // Attaching the Clerk JWT on every request
      Authorization: async () => {
        const token = await Clerk.session?.getToken(); 
        return token ? `Bearer ${token}` : null;
      }
    }
  }
});
In the code above, we configure the Supabase client to include the JWT from Clerk on every request. (The exact configuration might differ slightly; Clerk provides helpers too. For example, Clerk has an session.getToken() method we can use directly in Supabase’s accessToken option
supabase.com
.) The idea is that after a user logs in with Clerk, we retrieve their JWT and give it to Supabase. From then on, any call like supabase.from('Generated_Ads').select('*') will send the token. Supabase sees the token, verifies it (using Clerk’s public keys), and if valid, knows the requestor’s user ID. Then RLS kicks in to allow or deny data. This means we don’t have to write any additional code to filter data by user – the database does it for us securely. Note: The anon key we used in createClient is safe to include in the app – it can only perform actions allowed by RLS for “authenticated” or “anon” roles. We never expose the secret service key (that one is for your server admin tasks only). Also, in React Native (Expo), be sure to use AsyncStorage for the Supabase client’s auth storage as shown, so it doesn’t try to use browser localStorage. 6. Verify the Setup: After this integration, a typical request flow would be: The app calls, say, supabase.from('Bookmarks').select('*'). This triggers an HTTP request to Supabase with the Clerk token. Supabase parses the JWT and sees an authenticated user with some sub (user ID). The RLS policy on Bookmarks table ensures only rows where user_id = sub are returned
clerk.com
. Supabase then responds with just that user’s bookmarks. From the app’s perspective, it’s a simple query call – the heavy lifting of auth and filtering is done by Supabase/Postgres. If something is misconfigured (e.g., token missing or invalid), Supabase would return an error or no data. Therefore, ensure Clerk’s JWT is being sent and that your RLS policies are correctly written to match auth.jwt()->> 'sub' with the user_id fields.
Handling Images, Ads, and Bookmarks in Supabase
Now let’s discuss how we actually store and retrieve the various types of data in Adds-App using Supabase.
Storing Feed Template Images (Static Feed)
The feed consists of manually curated advertisement templates (images and maybe captions) that all users can see. We have a few options to manage these in Supabase:
Using Supabase Storage: The simplest approach is to upload all the template images to a Storage bucket (say, a bucket named templates). Each image will then have a URL (Supabase provides a CDN link or you can generate signed URLs). These images can be public (so the app can fetch them directly) or protected (requiring a token – but since these are not user-private, public read might be okay). Along with storing the files, we maintain the Templates table in the database with an entry for each template (which includes the file path or URL and any meta info). For example, template table row might look like: { id: 5, title: "Coffee Shop Ad", image_path: "templates/coffee.jpg", ... }. Because these are global, we typically leave RLS open for this table or create a read-only policy for all users (since everyone should fetch the feed).
Retrieving the Feed: To show the feed in the app, we would query this Templates table (e.g., supabase.from('Templates').select('*')). The result gives us the list of templates and the image paths/URLs. The app can then either directly use those URLs to display images or request the binary data. Usually, the app will just load the image from the URL (Supabase’s storage URLs can be used in an <Image> component in React Native, especially if public). If the storage requires a token, we can either make them public or use Supabase’s storage API to retrieve with the anon token. Given these images are not sensitive, making the bucket public simplifies things.
Because the feed is largely static and the same for everyone, this is a perfect scenario to apply caching (which we’ll detail in the Redis section). The first time a user (or any user) fetches the feed, we could store that list in a cache so that subsequent loads are instantaneous. However, even without caching, fetching a dozen rows and their URLs from Postgres is very fast (especially with ~1,000 users). Postgres can handle that easily, but caching can further reduce the tiny latency and avoid even hitting the DB for each user.
Storing Generated Ad Results
When users create new ads via the AI, we want to save those results both to display to the user and to allow them to revisit or share them later. Here’s how we handle generated ads:
Image Storage: Just like templates, the images produced by Gemini will be stored in a Supabase Storage bucket (perhaps a bucket named generated). The server (edge function) that gets the image from Gemini can upload it to this bucket. Each file could be named with a unique ID or a combination of user and timestamp, etc. Supabase returns a path or key for the uploaded file.
Database Record: We then insert a new row into the Generated_Ads table. This row contains the user_id (from the JWT or from the function context), the template_id that was used (if any), maybe the text prompt or some reference to the user’s input, and the output_image_path or URL where the image is stored. It’s also good to store a timestamp. This entry is now part of the user’s “history.” Thanks to RLS, the user will be able to query their own history easily, and they won’t see others’ entries. The overhead of storing these records is small (text and references). The heavy part (image file) is in object storage, which is separate and scalable. We should be mindful of overhead in the sense that if the user generates many images, we’ll accumulate many records and files. For 1,000 users, even if each generated, say, 10 images, that’s 10,000 records – Postgres can handle that volume without issue. The file storage might grow (10k images), but that’s fine as long as we have storage space (Supabase’s free tier has limits, but one can upgrade or clean up unused images). We might consider pruning or not storing images that aren’t saved by users to save space. For instance, if a user generates an image and doesn’t like it (doesn’t bookmark it), do we still keep it? We could keep it for a history log or delete it after some time to save space. This is a policy decision – for now, assume we keep history for completeness, but it’s something to consider for efficiency.
Retrieving History: The app can have a “My Creations” screen where it does supabase.from('Generated_Ads').select('*').order('created_at', {descending: true}). This will return the list of that user’s generated ads (with URLs to images). The user can scroll through their past creations. If we only want to show those they explicitly saved, we could filter by those that have a matching bookmark entry or mark a flag in the Generated_Ads table (like a boolean saved that we set when they bookmark it). But having a separate bookmarks table is more flexible (because you might save templates as well).
Managing Bookmarks (Saved Images)
For bookmarks, we use the Bookmarks table as described. Let’s walk through how adding and retrieving bookmarks might work:
Saving a Bookmark: When a user taps a “bookmark” or “save” icon on an image (whether it’s a feed template or one of their generated ads), the app will call a function to insert a new bookmark record. For example, if template #5 is bookmarked, we do supabase.from('Bookmarks').insert({ user_id: <currentUser>, image_type: 'template', image_ref_id: 5 }). Because we set up a default user_id value using the JWT and have an insert RLS policy, we might even omit user_id (Supabase can fill it from the token
clerk.com
) – but providing it explicitly is fine if we trust the client. If the JWT is attached, Supabase will check that user_id matches the token anyway. After insertion, we have a record like (id=..., user_id=u123, image_type='template', image_ref_id=5). If the user bookmarks their generated ad #17, similarly, we insert {user_id: u123, image_type:'generated', image_ref_id:17}.
Removing Bookmark: If the app has an option to un-save, it would simply delete that row from Bookmarks (or update a flag if we had one).
Viewing Bookmarks: To get all bookmarks for the user (for a “Saved” screen), we query Bookmarks table filtering on the current user. With RLS, we can just select all and it returns only that user’s rows. We’ll likely then need to join or fetch the actual image details those bookmarks refer to. For example, if we get a list of bookmarks with various image_ref_ids and types, our app could then fetch the actual images. We could do this in a single request by using Supabase’s ability to make RPC calls or multiple queries, but given a small number of bookmarks, a simple approach: for each bookmark, if image_type == 'template' then find that template in the Templates table (maybe we have it cached from the feed already!), or if image_type == 'generated' then find that entry in Generated_Ads (we could also store maybe the image URL directly in Bookmarks to avoid one extra lookup). To optimize, another design would be to include an image_url (or path) column in Bookmarks when inserting, so each bookmark row directly carries the URL to display. This avoids needing a join. The downside is data duplication (if the original image URL changes – which is unlikely). For simplicity, we might choose to duplicate the image URL in the bookmark record so that fetching bookmarks is one quick query and we get everything needed to display them (user id + image url + maybe a type).
Data relationships and modeling summary: We use the Clerk user ID as the primary way to link user-specific data. We enforce access with RLS. We store images in storage and references in tables. The tables we have (Templates, Generated_Ads, Bookmarks) cover the main use cases. All are in the public schema (default for Supabase) and can be accessed through the auto-generated API.
Caching with Redis for Performance (When and How)
With the basic functionality covered, let’s discuss how Redis can be introduced to improve performance. As the app scales beyond the initial 1,000 users, certain queries or data retrieval might become bottlenecks or unnecessarily repetitive. Caching is a strategy to store frequently accessed data in memory so that subsequent accesses are extremely fast (memory access vs. querying the database or recalculating things). Here’s how we can apply caching in Adds-App:
Why Use Redis?
While Postgres (Supabase) is quite performant, the Supabase API by default does not cache responses
github.com
. For data that is read often and rarely changes (like our feed templates), hitting the database every time is somewhat wasteful. Redis can act as a high-speed lookup for such data, reducing latency for the user and load on the DB. Redis is also useful for caching expensive computation results – in our case, an AI-generated image could be considered an expensive result (since calling Gemini AI might have a cost and time). If there’s a chance the same result will be needed again, caching it can avoid redoing the work.
What to Cache in Adds-App
Feed Templates: The feed is an ideal candidate for caching. We can cache the entire list of template metadata and perhaps even the images. For example, we could store a Redis key like feed:templates whose value is the JSON array of template objects (id, title, image_url, etc.). When the app requests the feed, our logic would be: check if feed:templates exists in Redis. If yes, return that immediately (the app gets the data with virtually no delay). If not, query Supabase (the Templates table), get the list, then store it in Redis for next time (and return it to the app). We might set an expiration (TTL) on this key for, say, an hour or a day, or refresh it whenever templates update. Since templates rarely change, we could even cache indefinitely and manually clear/update the cache when we add a new template. The benefit is huge if many users are loading the feed often – one database call can serve all users via the cached result.
Generated Ads Results: Caching user-specific generated results is trickier and might be optional. One scenario: if the user repeats a generation with identical input, we could save the previous result in Redis keyed by something like user:<id>:prompt:<hash> and if we see the same request again, return the cached result instantly instead of calling the AI again (saving time and cost). However, AI image generation is often nondeterministic – users might want a different result on a second try, not the same one. So caching at this level might not always make sense. Another possible use: after a generation is done and stored, we could cache that result for a short time so that if the user’s app needs to fetch it (to display or re-fetch image URL), it can get it from cache rather than hitting the database or storage. But since we will have just stored it, the data will also likely be in Postgres’s own cache and quick to retrieve. In summary, caching generated ads per user is optional and probably not critical unless we identify a specific need (like heavy analytics or reprocessing on those results). It’s okay to skip caching here initially and only add if a performance issue arises.
Other Cache Uses: We could also use Redis for things like counting feed views, rate limiting, or caching session data if needed. But focusing on our main data, the feed is the standout candidate.
Implementing Redis Cache
To use Redis, you need a running Redis server. You can use a managed service (like Upstash Redis which works great with serverless environments, or Redis Cloud) or host your own. Assuming we have a Redis instance URL and credentials, our app (or server functions) will connect to it. Since our React Native app is client-side, we typically would not connect to Redis directly from the app (we don’t want to expose Redis credentials and most mobile clients can’t maintain a Redis connection). Instead, the caching logic will live on the server side: e.g., within a Supabase Edge Function or any backend component that mediates data. Two possible patterns:
Using an Edge Function as a middle layer: We could write a Supabase Edge Function (in TypeScript or JavaScript running on Deno) that serves the feed. The app would call fetch('/edge-functions/getFeed') instead of directly hitting Supabase for the feed. That function would implement the logic: check Redis for feed:templates. If present, return it; if not, query Supabase (perhaps using the Supabase client or a SQL query), store in Redis, then return. This way, the caching is transparent to the client. The downside is a bit more complexity (you need to write and deploy the edge function) and a tiny bit more latency (the call goes to the function instead of directly to DB, but if the function is fast and near the DB, it’s negligible). The upside is you fully control caching and can integrate it nicely.
Using Postgres + Redis Foreign Data Wrapper: This is more advanced – Supabase’s database can actually be connected to Redis through a Foreign Data Wrapper (FDW)
supabase.com
. That means you could potentially create a Postgres table that maps to Redis data and use SQL to read/write cache. However, this is likely overkill and not beginner-friendly, so we don’t need to do this. It’s good to know Supabase has such capability, but for our purposes a simpler approach is better.
For our discussion, let’s assume we use an edge function or a simple Node backend to handle cached feed fetching. On app startup or feed refresh, instead of supabase.from('Templates').select('*'), we call our caching layer. Cache Invalidation: One important consideration is keeping the cache updated. For feed templates, if you add a new template or change one, the Redis cache would be stale. We have a couple of strategies:
Simply delete or update the cache entry whenever we make changes to the Templates table. If template updates are infrequent and done by us (the developers or admins), we can manually flush the feed:templates key via a small script or a function whenever we deploy new templates. We could also build an admin tool that when adding a template, also updates Redis.
Or use a short TTL (time-to-live) on the cache. For example, set the feed cache to expire every hour. That way, even if we forget to invalidate, it won’t be stale for long. Given our feed is manually curated, a manual invalidation approach is fine for now.
Trade-offs – Redis vs. Only Supabase: It’s worth noting the trade-offs of introducing Redis. On one hand, Redis can dramatically speed up certain operations (memory reads can be sub-millisecond) and reduce database load. On the other hand, it adds complexity: another service to maintain, and you must ensure cache consistency. If the app were small or the data not accessed often, we might skip Redis entirely. For ~1,000 users with periodic refresh, the database alone can handle things without breaking a sweat. Postgres will also cache frequently requested data in memory (its own buffer cache), so those feed queries likely get faster after the first few calls, even without Redis
github.com
. Thus, using Redis is an optimization – not strictly required for correctness, but beneficial as you scale or if you want the snappiest UX. We should also consider cost: if using a managed Redis, that might be an extra cost. If on a tight budget or for simplicity, one might decide to launch without Redis and see if performance is sufficient, then add it later if needed. In summary, store everything in Supabase for simplicity, add Redis caching for data that’s read-heavy and doesn’t change often to gain speed. Our case fits that for the feed.
Example: Caching the Feed
To solidify understanding, here’s a step-by-step of how we might implement and use the feed cache with Redis:
Initial Cache Load: We deploy the app and no cache exists yet. The first user opens the feed. The app calls our /getFeed endpoint (backed by an edge function).
Cache Miss: The function connects to Redis and checks for key feed:templates. It’s not there yet. So, the function queries the Supabase Templates table (this could be a direct SQL query or using the Supabase client with an admin key, since it’s a backend function). Suppose it retrieves an array of 10 templates and their data.
Store in Redis: The function then stores this array in Redis under feed:templates. It might store it as a JSON string. If using a JSON-capable Redis (RedisJSON module) or just a normal set, either way works. We could also cache each image binary, but that’s usually unnecessary since the client will fetch images via URL anyway (and those images themselves can be cached by HTTP caches or the device cache). We set an expiration maybe, or not, depending on strategy.
Return Response: The function returns the data to the app. The first user sees the feed (slightly slower because it had to hit the DB, but still quite fast, likely on the order of tens of milliseconds).
Subsequent Requests (Cache Hit): The next user (or the first user refreshing again) triggers the same function. Now Redis does have feed:templates. The function fetches from Redis and immediately returns the data, skipping the database. The latency here is very low – the feed appears almost instantly. This scales well even if hundreds of users refresh around the same time, since Redis can handle many concurrent reads easily and offloads the database.
Updating Feed: Later, we decide to add a new template image. We insert a new row in the Templates table via the Supabase dashboard or an admin interface. At this point, our cache is stale (it doesn’t have the new template). We then also connect to Redis (manually or via a small script or function) and either purge the feed:templates key or update it (e.g., fetch the new full list and set it again). Next time a user requests the feed, they’ll get the updated list (if we purged, it will miss and fetch from DB anew; if we updated, it’ll hit with the fresh data).
This approach ensures the feed is fast-loading. The combination of Supabase and Redis yields both persistency (Supabase storing the authoritative data) and performance (Redis speeding up reads).
Caching AI Results (Optional)
If we wanted to use Redis to cache AI generation results, one approach is to store the result of a given input for a short period. For instance, when a user triggers a generation, we could set a Redis key like gen_result:<user_id>:<hash_of_input> = resulting image URL. If the user for some reason triggers exactly the same generation again (say they accidentally closed the app and try again with the same parameters), the system could quickly return the cached image without calling Gemini again. This could save costs on the AI API. However, one must be careful: AI outputs might depend on randomness unless the API is deterministic or based on a prompt. If it’s deterministic (same input gives same output), caching makes sense to avoid duplicate work. If not, the user might expect a new result from the same input, so caching might confuse. We could bypass caching if a user explicitly requests a “new” generation. Another use: if generation takes a while, caching partial results or status can help. But that might be beyond our scope (that’s more for job queues). In conclusion, caching for generation is a “nice-to-have” optimization and can be revisited once the core features are stable. To start, focus Redis on the feed where it clearly shines.
Supabase Advanced Features: Realtime (Multiplayer) and MCP (AI Integration)
Supabase offers some advanced capabilities that go beyond basic CRUD storage. Two notable ones are the Realtime API (Multiplayer features) and the newer MCP server (Model Context Protocol). We’ll explain these in context and whether they’re relevant to Adds-App.
Supabase Realtime (Multiplayer Coordination)
Supabase’s Realtime service is a built-in WebSocket engine that can broadcast changes in the database to clients and also support presence and custom messaging channels
supabase.com
. This essentially enables “multiplayer” or collaborative features (similar to how Firebase Realtime DB or Firestore send updates). In Supabase Realtime (recently updated as “Realtime: Multiplayer Edition”), everything is organized into channels (like chat rooms). Clients can subscribe to a channel to receive messages or database change events in real-time
supabase.com
. Two key features here are:
Broadcast: You can broadcast arbitrary messages to a channel – for example, send a notification or update to all users listening on “feedUpdates” channel. If we were pushing new ads to the feed dynamically, we could use this for instant updates instead of periodic refresh. In our case, the feed refresh is not live, so we might not use broadcast right now. But if one day you want users to see new template images the moment we add them, a broadcast could notify the app to fetch the latest feed.
Presence: This allows tracking which users or how many are online in a channel, and possibly their state (for example, in a collaborative context, you could show avatars of users viewing the same item, etc.). For Adds-App, presence isn’t particularly needed because users aren’t interacting in real-time with each other within the app (it’s more of a single-user experience with content generation).
When Realtime is relevant: If we introduce social or collaborative features (imagine users could share generated ads in a common gallery or comment on them), then realtime updates would be useful to sync that across clients. Another example – if we had an admin dashboard that needed to monitor generation events live, we could set up a subscription to the Generated_Ads table. Each time a new row is inserted (new ad generated), the admin interface could immediately show it. Under the hood, Supabase’s Realtime server listens to Postgres changes (via logical replication) and forwards them to subscribers
supabase.com
supabase.com
. It’s a powerful feature but comes with complexity (you have to manage WebSocket connections in the app). For now, our app expects periodic manual refresh, which might be simpler. But it’s good to know that Supabase can do realtime “multiplayer” coordination out of the box if the need arises. This sets it apart from a plain Postgres – it’s already configured to push updates and handle concurrent clients listening to data. To clarify terminology: The user question mentioned “Supabase’s MCP (Multiplayer Coordination Protocol) server”. This seems to conflate the realtime multiplayer features with MCP. The multiplayer aspect is covered by Supabase Realtime (presence/broadcast), whereas MCP actually stands for Model Context Protocol, which is something different (related to AI). So, let’s explain MCP next to avoid confusion.
MCP Server (Model Context Protocol) for AI Integration
The MCP server in Supabase is a relatively new addition designed to interface with AI tools. MCP stands for Model Context Protocol, a standard that allows Large Language Models (LLMs) or AI assistants to communicate with external services like databases
supabase.com
. Supabase released an MCP server so that tools like OpenAI’s ChatGPT (via plugins or tool usage), Anthropic’s Claude, or other AI agents can directly perform operations on your Supabase project in a controlled way
supabase.com
. For example, with the MCP server, an AI agent could create tables, run SQL queries, or manage your Supabase project if you allow it
supabase.com
supabase.com
. How is this relevant to Adds-App? In the current app functionality, MCP is not directly needed for end-users. MCP is more of a developer or power-user tool – it’s like saying you could chat with an AI and tell it to “fetch all users who generated more than 5 ads” and it would use MCP to run that query on the database and give you an answer. For our architecture, the benefit of MCP might be indirect: since we are using an AI (Gemini) to generate content, one might imagine a scenario where an AI agent helps coordinate something in the app or manage content. For instance, maybe in the future you integrate an AI assistant in the app that can analyze the user’s past ads and give marketing tips – that assistant could use MCP to safely query the Supabase DB for the user’s history (with permission) and then provide advice. Or as developers, we could use an AI coding assistant that, via MCP, helps us manage the database (like run migrations or find a bug by querying data). To be clear, MCP is not required to use Google Gemini or to store data – it’s an optional tool that standardizes how an AI model could interact with Supabase. Supabase’s MCP server basically provides a set of “tools” the AI can use, such as running a SQL query or creating a row
supabase.com
, under the hood. If you’re curious: Supabase’s MCP server was announced in April 2025 as a way to connect AI development tools with your database. Unless you plan to have an AI agent directly manage or query your database, you won’t need to run an MCP server for your app’s normal operations. When could MCP be relevant? If Adds-App had a feature like “AI analytics” where the user could ask questions in natural language about their generated ads (like “Which of my ads got the most bookmarks?”) and you wanted an AI to interpret that and fetch the answer from the DB, MCP could be the bridge that lets the AI securely query the database. Another use: as a developer, you could use an AI coding assistant integrated with Supabase (for instance, in your IDE or a tool like Cursor) to generate code or SQL for your project; the MCP server helps that assistant talk to your Supabase instance safely
supabase.com
. In short, Supabase’s architecture includes this MCP component for AI interoperability, but it’s not something you must utilize in building the core features of Adds-App. It’s good to be aware of it (especially since you’re using cutting-edge AI like Gemini – the ecosystem is converging such that your AI and database can communicate). But for our current scope (user feed, bookmarks, storing images), you won’t be dealing with MCP directly unless you have a specific AI agent use-case. Focus on the core pieces (DB, auth, caching), and consider MCP as a potential enhancement for AI-driven functionality or dev tooling.
Putting It All Together: Workflow Example
To solidify the understanding, let’s walk through a typical user story in Adds-App and see how Supabase, Clerk, and Redis work together behind the scenes: 1. User Signs Up/In: The user opens the app and is presented with a login/signup (powered by Clerk’s UI). Suppose the user signs up. Clerk securely creates their account (email/password or social login) and once authenticated, the app now holds a Clerk session token (JWT) for that user. We configure the Supabase client with this token, so the user is effectively “logged in” to our backend as well. (Meanwhile, in Supabase, we did not create a new row in an auth table – instead, Supabase will trust Clerk’s token for identification. We might use a Clerk webhook to create a profile row in a users table if we wanted, but it’s optional). 2. Loading the Feed: The user enters the main feed screen. The app needs the list of template ads. Instead of directly calling Supabase for the Templates table, our app might call an endpoint (e.g., we could create a small API or use an edge function as discussed). That endpoint checks the Redis cache for the feed. Let’s say this is not the very first time, so the cache is warm – Redis returns the list of templates immediately. The app receives the JSON of templates (with image URLs) and then for each item, it loads the image from the URL (which might be something like https://<project>.supabase.co/storage/v1/object/public/templates/coffee.jpg). Those images might be cached by the device or CDN as well, making it quick. The feed is displayed to the user with minimal delay. If the cache had been empty, the service would fetch from Supabase’s database (which, for a few rows, is still very quick)
github.com
. In either case, the user sees the feed content. 3. User Selects a Template to Create an Ad: The user taps on a template and then uploads their own image or enters some text for the Gemini AI to generate a customized ad. When they hit “Generate,” the app calls a cloud function (for example, a Supabase Edge Function we wrote called generateAd). This function receives the request along with the user’s Clerk JWT (or we design it such that only authorized users can invoke it – Supabase functions can read the JWT context automatically). The function then does a few things:
It verifies the user/request (Supabase functions have access to the JWT and can even enforce RLS-like checks if we implement them, but since this is a custom function, we ensure the user is logged in).
It calls the Google Gemini AI API with the provided input. This could involve sending the user’s image and the template ID or style instructions. (We’d have stored something about the template’s style in the Templates table perhaps, or the function can fetch the template info if needed to pass to AI).
After some processing time, Gemini returns the generated advertisement image (let’s assume as a binary or URL).
The function then stores the result: it uploads the image file to the Supabase Storage bucket generated (we might generate a unique filename, e.g., <userId>_<timestamp>.png). Supabase returns a path or public URL for the uploaded file. Next, the function inserts a new row into Generated_Ads table with user_id = this user, template_id = chosen template, and output_image_path = that file path, plus a timestamp. Because this function is running with admin privileges (it could use the service key internally), it can insert directly. The row is now in the database.
Optionally, the function could also put this result into a Redis cache. For example, set a key last_result:<user_id> = the file URL. This might not be necessary, but if we want the app to poll for the result instead of the function waiting synchronously, a cache could be a place to store the result for pickup. However, assuming the function call is synchronous (the app waits for the response), we might skip caching here and just return the data.
The function returns a response to the app, containing, say, the URL of the generated image and maybe the new record’s ID or other metadata.
4. User Sees the Generated Ad: The app now takes the returned URL and displays the new ad image to the user. The user can now choose to bookmark it if they like. At this point, the image is persisted in Supabase Storage and the creation logged in the DB. If the user closes the app, the result is not lost – they can find it in their history later. 5. User Bookmarks the New Ad: The user taps the bookmark icon on their newly generated ad. The app will then call Supabase (using the JS client) to insert into the Bookmarks table. Thanks to our setup, we simply do something like:
js
Copy
Edit
supabase.from('Bookmarks').insert({
  image_type: 'generated',
  image_ref_id: newAdId   // the ID of the generated ad record, or we could use the image path
});
We don’t even need to send user_id if we set up the default correctly; but even if we do, Supabase will validate it against the JWT. This request goes to Supabase’s PostgREST endpoint. RLS policy allows it (since the JWT is attached and it matches the inserted user_id). A new bookmark row is created in the database. Now the user’s bookmarks include this item. 6. Viewing Saved Bookmarks: Suppose the user goes to their “Saved Ads” screen. The app will fetch from Bookmarks table (and possibly join or separately fetch the actual images). We could have a view or use Supabase’s select() with foreign tables if we had set up relationships. Simpler: query Bookmarks where user_id = current (with RLS, just select all). The app gets an array of bookmarks. For each, it knows if it’s a template or generated. It then either already has the template info (from feed cache) or it queries the Generated_Ads table for those IDs (could do a batch request). The end result is the app now has all the data to show the user’s bookmarked images. They click one and can view it (via the URL stored). 7. Periodic Feed Refresh: The next day, the user pulls to refresh the feed. The app calls the feed endpoint again. If we updated the feed with a new template, our backend cache was updated, so Redis now serves a slightly different list (with the new item). The user gets the latest feed. If nothing changed, it’s the same data (possibly served from cache, still fast). Because we are not using realtime push, the user only sees updates when they refresh – which in our scenario is acceptable. 8. Scale and performance: With ~1,000 users performing these actions, let’s consider the load: Auth is handled by Clerk (which can easily handle that user count). Each user might generate a few images – so maybe a few thousand records and storage objects in total, which Postgres and Supabase Storage can handle on a free/pro tier without issue. The feed reads – if done without caching, that’s 1,000 queries which is fine; with caching, it might be just a handful of queries and the rest served from memory. The most intensive part might be the image generation calls to Gemini (which is outside our stack’s control but presumably can handle concurrent requests or you may queue them). Supabase’s database will mainly be handling relatively small read/write operations (inserting a new row for each generation or bookmark, selecting lists, etc.). Postgres is very capable of this scale on commodity hardware. Additionally, Supabase’s infrastructure likely runs connection pooling and optimizations so that these requests are efficient. We also have the advantage of Postgres’s reliability and correctness (with transactions, etc.), so our data remains consistent. 9. Security considerations: By using Clerk and RLS, we ensured that even if someone tampered with the app, they couldn’t access other users’ data via the API. Without the proper JWT, Supabase would treat them as anon (and our policies would deny access). The integration of Clerk’s JWT means we didn’t have to build our own user management or worry about password storage. Clerk handles that, and Supabase just trusts the JWT signature (Clerk’s public JWKS). Also, Supabase Storage can be configured so that only authorized users can access certain buckets. For instance, we might make the generated bucket private so that users can’t randomly fetch others’ images by guessing URLs. Instead, our app would request a signed URL for the file via Supabase if needed. However, since RLS doesn’t directly cover storage, a simple approach for now is to use somewhat unguessable paths or make the bucket public but the file names obscure. If this were sensitive, we’d tighten it (Supabase allows generating signed URLs that expire, etc.). For template images, we chose to make them public for ease of use. 10. Maintenance: As developers, we’ll use Supabase Studio (the web dashboard) to monitor the tables, possibly run queries (like see how many ads were generated this week), and manage storage. We can also integrate Supabase with other tools; for example, using webhooks or functions to notify us of certain events (if needed). Clerk provides a dashboard for user management (so we can see our user base, handle support, etc.). If we need to manually moderate or remove content, we can do so through the database or storage (e.g., if a generated image violated some terms, we could delete that record and file).
Conclusion
By combining Supabase, Clerk, and Redis, we achieve a robust yet flexible backend for Adds-App:
Supabase serves as the all-in-one backend for data persistence (using Postgres), file storage for images, and even serverless functions for custom logic like calling the AI. Its rich feature set (RLS, Realtime, etc.) allows us to implement strong security and consider future expansions (like live updates) with minimal effort. We leverage its Postgres foundation for reliable data integrity and use its RESTful API via the Supabase JS client to interact from our React Native app. Supabase’s architecture ensures we aren’t limited to just a key-value store; we can use SQL and relational modeling which is great for structured data like our relationships between users, images, and bookmarks. And if we needed to, we could directly access the database or run analytical queries since it’s a real Postgres under the hood.
Clerk cleanly handles the user authentication flow. Users get a smooth sign-up/login experience with modern features (social logins, email verification, etc., depending on what we enable in Clerk). For us, integrating Clerk with Supabase means we don’t use Supabase’s own auth system, but that’s fine – Supabase was designed to let you bring your own auth if you want
supabase.com
. We configured Supabase to accept Clerk’s JWTs, and our RLS policies use those JWT claims to secure data. This separation of concerns is powerful: Clerk manages identities, Supabase manages data, and they communicate via standards (JWT). The result is improved security (JWTs for each request
clerk.com
) and a scalable way to ensure each user can only access their data.
Redis adds a layer of performance optimization. While not strictly necessary at low scale, it positions us well as we grow. By caching static or frequently accessed content (like the feed templates), we drastically reduce response times and database hits. Users will feel the app is more “instant” because cached responses avoid even the minor delay of a database query. We also discussed caching AI results for efficiency, though that may be used sparingly. It’s important to monitor and profile the app – if we find certain queries are slow or happening too often, we can plug in a cache like Redis to alleviate that. It’s a classic trade-off of speed vs. complexity, but we’ve scoped it to just the feed mainly, which is manageable.
Scalability for 1,000+ users: The chosen architecture should comfortably handle the first 1,000 users and beyond. Supabase in its free or base tiers can handle multiple concurrent connections and operations; Postgres can scale vertically for a while (and Supabase can upgrade the instance if needed). Clerk can handle up to tens of thousands of users on their free plan as well (and more on paid). Redis (if used via a service like Upstash) can handle our relatively small data with minimal cost. If the app grows to say 100k users, we’d revisit our provisioning (making sure the database has enough RAM/CPU, enabling read replicas or using Supabase’s caching add-ons if any, and so on). But the architecture itself – using decoupled services and standard protocols – will scale: we can add more edge functions, more Redis nodes, or even move to a CDN for certain assets if needed, without a complete overhaul.
Finally, we touched on Supabase’s realtime and MCP features which, while not needed immediately, give us a glimpse of future possibilities (like live updates or AI-driven interactions with the database). As a beginner, you can focus on getting the core flows working with Supabase, Clerk, and (optionally) Redis as described. The key steps are: set up your Supabase tables and storage with proper RLS, integrate Clerk for auth, use the Supabase client in your Expo app with Clerk’s token, and consider an edge function or small backend for caching if you want extra speed. Once set up, you’ll have a modern backend where the heavy lifting (auth and database) is largely managed for you, and you can concentrate on building a great user experience in the app. Good luck with Adds-App, and enjoy the benefits of this powerful stack!

# mermaid diagram :

flowchart LR
  subgraph "Mobile App\n(React Native + Clerk + Supabase Client)"
    UI[App UI]
  end

  subgraph "Authentication Service"
    Clerk[Clerk Auth]
  end

  subgraph "Cache Layer"
    Redis[Redis Cache]
  end

  subgraph "Backend (Supabase)"
    subgraph "Edge Functions"
      GetFeed[getFeed Function]
      GenAd[generateAd Function]
    end
    subgraph "Database"
      Templates[Templates Table]
      Generated[Generated_Ads Table]
      Bookmarks[Bookmarks Table]
    end
    Storage[Supabase Storage]
  end

  subgraph "AI Service"
    Gemini[Google Gemini AI]
  end

  %% Auth Flow
  UI -->|Sign In / Sign Up| Clerk
  Clerk -->|Issue JWT| UI

  %% Feed Flow
  UI -->|Fetch Feed (with JWT)| GetFeed
  GetFeed -->|Check Cache| Redis
  Redis -->|Cache Miss| GetFeed
  GetFeed -->|Query Templates| Templates
  Templates -->|Return Data| GetFeed
  GetFeed -->|Store in Cache| Redis
  GetFeed -->|Return Feed| UI

  %% Ad Generation Flow
  UI -->|Generate Ad (with JWT)| GenAd
  GenAd -->|Call API| Gemini
  Gemini -->|Return Image| GenAd
  GenAd -->|Upload Image| Storage
  Storage -->|URL| GenAd
  GenAd -->|Insert Record| Generated
  GenAd -->|Return URL| UI

  %% Bookmark & History
  UI -->|Bookmark Image (with JWT)| Bookmarks
  UI -->|View History| Generated
  UI -->|View Bookmarks| Bookmarks
